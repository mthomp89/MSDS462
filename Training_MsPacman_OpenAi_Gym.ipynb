{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_MsPacman_OpenAi_Gym.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mthomp89/MSDS462/blob/master/Training_MsPacman_OpenAi_Gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Training_MsPacman_OpenAi_Gym  \n",
        "\n",
        "# Install Dependencies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Pacman Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnl02X8cY49D",
        "colab_type": "text"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)\n",
        "\n",
        "# check the observation space\n",
        "print(env.observation_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation = env.reset()\n",
        "counter = 0\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "\n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action)\n",
        "    counter += 1\n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "print(\"There were {} timesteps\".format(counter))\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH7CYfgZdq4d",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Network  \n",
        "Sourced from: https://github.com/moduIo/Deep-Q-network/blob/master/DQN.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky6YEtzpwiow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PIKrXeOxD0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN_Agent:\n",
        "    #\n",
        "    # Initializes attributes and constructs CNN model and target_model\n",
        "    #\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.9            # Discount rate\n",
        "        self.epsilon = 1.0          # Exploration rate\n",
        "        self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
        "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
        "        self.update_rate = 1000     # Number of steps until updating the target network\n",
        "        \n",
        "        # Construct DQN models\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.model.summary()\n",
        "\n",
        "    #\n",
        "    # Constructs CNN\n",
        "    #\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        \n",
        "        # Conv Layers\n",
        "        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n",
        "        model.add(Activation('relu'))\n",
        "        \n",
        "        model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        \n",
        "        model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        # FC Layers\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        \n",
        "        model.compile(loss='mse', optimizer=Adam())\n",
        "        return model\n",
        "\n",
        "    #\n",
        "    # Stores experience in replay memory\n",
        "    #\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    #\n",
        "    # Chooses action based on epsilon-greedy policy\n",
        "    #\n",
        "    def act(self, state):\n",
        "        # Random exploration\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        \n",
        "        act_values = self.model.predict(state)\n",
        "        \n",
        "        return np.argmax(act_values[0])  # Returns action using policy\n",
        "\n",
        "    #\n",
        "    # Trains the model using randomly selected experiences in the replay memory\n",
        "    #\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        \n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            \n",
        "            if not done:\n",
        "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
        "            else:\n",
        "                target = reward\n",
        "                \n",
        "            # Construct the target vector as follows:\n",
        "            # 1. Use the current model to output the Q-value predictions\n",
        "            target_f = self.model.predict(state)\n",
        "            \n",
        "            # 2. Rewrite the chosen action value with the computed target\n",
        "            target_f[0][action] = target\n",
        "            \n",
        "            # 3. Use vectors in the objective computation\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "            \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    #\n",
        "    # Sets the target model parameters to the current model parameters\n",
        "    #\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "            \n",
        "    #\n",
        "    # Loads a saved model\n",
        "    #\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    #\n",
        "    # Saves parameters of a trained model\n",
        "    #\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLD0Yr_YAYPM",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktQXJ6lrx_dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helpful preprocessing taken from github.com/ageron/tiny-dqn\n",
        "def process_frame(frame):\n",
        "    mspacman_color = np.array([210, 164, 74]).mean()\n",
        "    img = frame[1:176:2, ::2]    # Crop and downsize\n",
        "    img = img.mean(axis=2)       # Convert to greyscale\n",
        "    img[img==mspacman_color] = 0 # Improve contrast by making pacman white\n",
        "    img = (img - 128) / 128 - 1  # Normalize from -1 to 1.\n",
        "    \n",
        "    return np.expand_dims(img.reshape(88, 80, 1), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtH0j7WfyFS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def blend_images(images, blend):\n",
        "    avg_image = np.expand_dims(np.zeros((88, 80, 1), np.float64), axis=0)\n",
        "\n",
        "    for image in images:\n",
        "        avg_image += image\n",
        "        \n",
        "    if len(images) < blend:\n",
        "        return avg_image / len(images)\n",
        "    else:\n",
        "        return avg_image / blend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62E50WmOAfSD",
        "colab_type": "text"
      },
      "source": [
        "## Hyperperameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn3ZMoUOyHmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_size = (88, 80, 1)\n",
        "action_size = env.action_space.n\n",
        "agent = DQN_Agent(state_size, action_size)\n",
        "\n",
        "episodes = 10\n",
        "batch_size = 8\n",
        "skip_start = 90  # MsPacman-v0 waits for 90 actions before the episode begins\n",
        "total_time = 0   # Counter for total number of steps taken\n",
        "all_rewards = 0  # Used to compute avg reward over time\n",
        "blend = 4        # Number of images to blend\n",
        "done = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPziDIJGAuHp",
        "colab_type": "text"
      },
      "source": [
        "## Model Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onarxgbqyLiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "high_score = 0\n",
        "\n",
        "for e in range(episodes):\n",
        "    total_reward = 0\n",
        "    game_score = 0\n",
        "    state = process_frame(env.reset())\n",
        "    images = deque(maxlen=blend)  # Array of images to be blended\n",
        "    images.append(state)\n",
        "    \n",
        "    for skip in range(skip_start): # skip the start of each game\n",
        "        env.step(0)\n",
        "    \n",
        "    for time in range(20000):\n",
        "        env.render()\n",
        "        total_time += 1\n",
        "        \n",
        "        # Every update_rate timesteps we update the target network parameters\n",
        "        if total_time % agent.update_rate == 0:\n",
        "            agent.update_target_model()\n",
        "        \n",
        "        # Return the avg of the last 4 frames\n",
        "        state = blend_images(images, blend)\n",
        "        \n",
        "        # Transition Dynamics\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Return the avg of the last 4 frames\n",
        "        next_state = process_frame(next_state)\n",
        "        images.append(next_state)\n",
        "        next_state = blend_images(images, blend)\n",
        "        \n",
        "        # Store sequence in replay memory\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        game_score += reward\n",
        "        reward -= 1  # Punish behavior which does not accumulate reward\n",
        "        total_reward += reward\n",
        "        if game_score > high_score:\n",
        "            high_score = game_score\n",
        "        \n",
        "        if done:\n",
        "            all_rewards += game_score\n",
        "            \n",
        "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
        "                  .format(e+1, episodes, game_score, total_reward, all_rewards/(e+1), time, total_time))\n",
        "            \n",
        "            break\n",
        "            \n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGLZcVL6A_ts",
        "colab_type": "text"
      },
      "source": [
        "## High Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lni2NA_ty9qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(high_score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}